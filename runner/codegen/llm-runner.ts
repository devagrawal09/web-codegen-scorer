import { z } from 'zod';
import { LlmResponseFile, Usage } from '../shared-interfaces.js';
import { UserFacingError } from '../utils/errors.js';

export function assertValidModelName(value: string, availableModels: string[]) {
  if (!availableModels.includes(value)) {
    throw new UserFacingError(
      `Unsupported model specified. Available models:\n` +
        availableModels.map((m) => `- ${m}`).join('\n')
    );
  }
}

/**
 * Interface for running a large language model.
 */
export interface LlmRunner {
  /** Unique ID of the runner. */
  readonly id: string;

  /** Display name of the runner. */
  readonly displayName: string;

  /**
   * Whether the runner is able to run a repair loop on its own or if it
   * should be triggered by the eval infrastructure via prompting.
   */
  readonly hasBuiltInRepairLoop: boolean;

  /** Sends a file generation request to the LLM. */
  generateFiles(
    options: LlmGenerateFilesRequestOptions
  ): Promise<LlmGenerateFilesResponse>;

  /** Sends a normal text generation request to the LLM. */
  generateText(
    options: LlmGenerateTextRequestOptions
  ): Promise<LlmGenerateTextResponse>;

  /** Sends a schema-constrained generation request to the LLM. */
  generateConstrained<T extends z.ZodTypeAny = z.ZodTypeAny>(
    options: LlmConstrainedOutputGenerateRequestOptions<T>
  ): Promise<LlmConstrainedOutputGenerateResponse<T>>;

  /** Gets the names of the models supported by the runner. */
  getSupportedModels(): string[];

  /**
   * Starts all of the specified MCP servers.
   * Optional since not all runners may support MCP.
   * @param hostName Name for the MCP host.
   * @param servers Configured servers that should be started.
   */
  startMcpServerHost?(hostName: string, servers: McpServerOptions[]): void;

  /** Stops tracking MCP server logs and returns the current ones. */
  flushMcpServerLogs?(): string[];

  /** Cleans up any resources taken up by the runner. */
  dispose(): Promise<void>;
}

interface BaseLlmRequestOptions {
  /** Name of the model to use. */
  model: string;
  /** Whether to skip sending the requests to the running MCPs. */
  skipMcp?: boolean;
  /** Optional messages to be passed along as context. */
  messages?: PromptDataMessage[];
  /** Configures a timeout for the request. */
  timeout?: {
    /** Message to be logged if the request times out. */
    description: string;
    /** Duration of the timeout in minutes. */
    durationInMins: number;
  };
  /** Configuration for the "thinking mode" of a model. */
  thinkingConfig?: {
    /** Whether to capture thoughts and return them.  */
    includeThoughts?: boolean;
  };
  /** Signal to fire when this LLM request should be aborted. */
  abortSignal: AbortSignal;
}

/** Options needed to send a text generation request. */
export interface LlmGenerateTextRequestOptions extends BaseLlmRequestOptions {
  /** Prompt to send. */
  prompt: string;
}

/** Context needed for an file generation context. */
export interface LlmGenerateFilesContext {
  /** System instructions that should be included. */
  systemInstructions: string;
  /** Prompt being executed. */
  executablePrompt: string;
  /**
   * Combined system instructions and prompt for the environments
   * where the two can't be provided separately.
   */
  combinedPrompt: string;
  /** Directory in which the generation will occur. */
  directory: string;
  /** Command that the LLM can use to verify that the build works. */
  buildCommand: string;
  /** Package manager that the LLM can use. */
  packageManager: string;
  /** All available package managers supported by the runner. */
  possiblePackageManagers: string[];
}

/** Options needed to send a file generation request. */
export interface LlmGenerateFilesRequestOptions extends BaseLlmRequestOptions {
  /** Context necessary for the request. */
  context: LlmGenerateFilesContext;
}

/**
 * Options that can be passed for a schema-constrained generation
 * request to an LLM.
 */
export interface LlmConstrainedOutputGenerateRequestOptions<
  T extends z.ZodTypeAny = z.ZodTypeAny,
> extends BaseLlmRequestOptions {
  /** Prompt to send. */
  prompt: string;
  /** Schema that the response should conform to. */
  schema: T;
}

/** Constrained output response by the LLM. */
export interface LlmConstrainedOutputGenerateResponse<
  T extends z.ZodTypeAny = z.ZodTypeAny,
> {
  /** Result generated by the LLM. */
  output: z.infer<T> | null;
  /** Token usage data, if available. */
  usage?: Partial<Usage>;
  /** Reasoning messages from the LLM. */
  reasoning: string;
}

/** File generation response from the LLM. */
export interface LlmGenerateFilesResponse {
  files: LlmResponseFile[];
  /** Token usage data, if available. */
  usage?: Partial<Usage>;
  /** Reasoning messages from the LLM. */
  reasoning: string;
}

/** Text response from the LLM. */
export interface LlmGenerateTextResponse {
  text: string;
  /** Token usage data, if available. */
  usage?: Partial<Usage>;
  /** Reasoning messages from the LLM. */
  reasoning: string;
}

/** Schema for the LLM server options. */
export const mcpServerOptionsSchema = z.object({
  /** Name of the server. */
  name: z.string(),
  /** Command that starts the server. */
  command: z.string(),
  /** Arguments to pass into the command. */
  args: z.array(z.string()),
  /** Environment variables to use when executing the command */
  env: z.record(z.string(), z.string()).optional(),
});

/** Options used to start an MCP server. */
export type McpServerOptions = z.infer<typeof mcpServerOptionsSchema>;

/**
 * Type for a prompt message may be passed to LLM runner in the eval tool.
 *
 * A more specialized type allows us to count tokens without having to
 * deal with conversions from complex prompt data to e.g. OpenAI message data.
 * */
export interface PromptDataMessage {
  role: 'user';
  content: Array<
    { text: string } | { media: { url: string; base64PngImage: string } }
  >;
}
