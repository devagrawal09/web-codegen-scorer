import type { BuildErrorType, BuildResult } from './builder/builder-types.js';
import type { REPORT_VERSION } from './configuration/constants.js';
import type { UserJourneysResult } from './orchestration/user-journeys.js';
import type { AutoRateResult } from './ratings/autoraters/auto-rate-shared.js';
import type { Rating, RatingCategory } from './ratings/rating-types.js';

/**
 * Represents a single prompt definition and extra metadata for it.
 */
export interface PromptDefinition {
  /**
   * A descriptive name for the prompt, used for identification (e.g., file naming).
   */
  name: string;

  /**
   * Kind of the prompt.
   */
  readonly kind: 'single';

  /**
   * System prompt type needed for this prompt. E.g. `generation` for the first step
   * of a multi-step prompt, or `editing` for subsequent steps.
   */
  systemPromptType: 'generation' | 'editing';

  /**
   * The actual text of the prompt to be sent to the language model.
   */
  prompt: string;

  /**
   * Addition ratings specific to this definition.
   */
  ratings: Rating[];

  /**
   * Relative paths to files that should be passed as context to LLM calls.
   */
  contextFilePatterns: string[];
}

/**
 * Represents a prompt that is made up of steps that are executed in order.
 */
export interface MultiStepPromptDefinition {
  /**
   * A descriptive name for the prompt, used for identification (e.g., file naming).
   */
  name: string;

  /**
   * Kind of the prompt.
   */
  readonly kind: 'multi-step';

  /**
   * Individual steps in the form of prompts, in the order in which they should be executed.
   */
  steps: PromptDefinition[];
}

/** Possible type definitions of a root prompts. */
export type RootPromptDefinition = PromptDefinition | MultiStepPromptDefinition;

/**
 * Usage data from the LLM call.
 */
export interface Usage {
  /** Number of input tokens used. */
  inputTokens: number;
  /** Number of output tokens produced. */
  outputTokens: number;
  /**
   * Number of total tokens involved.
   *
   * This number can be different from `input + output`. Presumably
   * due to e.g. thinking process of models. See:
   * https://ai.google.dev/gemini-api/docs/thinking.
   * */
  totalTokens?: number;
}

/**
 * File generated by the LLM.
 */
export interface LlmResponseFile {
  /** Relative path of the file within the project. */
  filePath: string;
  /** Source code of the file. */
  code: string;
}

/**
 * Response from an LLM call.
 */
export interface LlmResponse {
  /** Files generated by the LLM. */
  outputFiles: LlmResponseFile[];
  /** Whether the call was successful. */
  success: boolean;
  /** API errors that were returned. */
  errors: string[];
  /** Token usage data for the call. */
  usage: Usage;
  /** Reasoning messages from the LLM for generating this response. */
  reasoning: string;
  /** Tool requests logs (e.g. MCP requests and responses). */
  toolLogs: ToolLogEntry[];
}

/** Error response from an LLM API. */
export interface LlmResponseError {
  /** Name of the prompt that failed. */
  promptName: string;
  /** Error message */
  message: string;
}

/** File that can be passed as context to an LLM. */
export interface LlmContextFile {
  /** Relative of the file. */
  relativePath: string;
  /** Content of the file. */
  content: string;
}

/** Group of assessments in the final report. */
export interface AssessmentCategory {
  /** Unique ID of the category. */
  id: RatingCategory;
  /** Display name of the cateogry. */
  name: string;
  /** Points that have been awarded to the category. */
  points: number;
  /** Maximum number of points the category can have. */
  maxPoints: number;
  /** Assessments that make up the score of the category. */
  assessments: (IndividualAssessment | SkippedIndividualAssessment)[];
}

/** Possible states of an assessment. */
export enum IndividualAssessmentState {
  EXECUTED,
  SKIPPED,
}

/**
 * Represents the result of a single assessment check performed on the code.
 */
export interface IndividualAssessment {
  state: IndividualAssessmentState.EXECUTED;
  /** The name of the assessment check. */
  name: string;
  /** A brief description of what the check does. */
  description: string;
  /** Unique ID of the assessment check. */
  id: string;
  /** Category under which the check falls. */
  category: RatingCategory;
  /**
   * Score assigned for the current check between 0 and 1.
   * 0 means that the app failed while 1 means that it passed fully.
   */
  successPercentage: number;
  /** Fraction of the total score that this check would reduce, if it was failing. */
  scoreReduction: `${number}%`;
  /** A message detailing the outcome of the check. */
  message: string;
  /** LLM usage for running the assessment. */
  usage?: Usage;
}

export interface SkippedIndividualAssessment {
  state: IndividualAssessmentState.SKIPPED;
  /** The name of the assessment check. */
  name: string;
  /** A brief description of what the check does. */
  description: string;
  /** Unique ID of the assessment check. */
  id: string;
  /** Category under which the check falls. */
  category: RatingCategory;
  /** A message explaining why the check was skipped. */
  message: string;
}

/**
 * Represents the overall score and breakdown of code assessments.
 */
export interface CodeAssessmentScore {
  /** The total points accumulated from all passed checks. */
  totalPoints: number;
  /** Max amount of points if all assessments are successful. */
  maxOverallPoints: number;
  categories: AssessmentCategory[];
  /**
   * LLM usage that was involved for running the code assessments.
   * Older reports might not have this set.
   */
  tokenUsage?: Usage;
}

/**
 * Details for each attempt made during code generation and repair.
 * This includes the generated code, the result of the build attempt, and the attempt number.
 */
export interface AttemptDetails {
  /** The generated source code for this attempt. */
  outputFiles: LlmResponseFile[];
  /** The result of the build process for this attempt's code. */
  buildResult: BuildResult;
  /** The sequential number of this attempt (0 for initial generation, 1+ for repairs). */
  attempt: number;
  /** Captures usage details (# of input and output tokens) */
  usage: Usage;
  /** LLM reasoning messages for generating these files. */
  // Note: May not be set in older reports.
  reasoning?: string;
}

/** Statistics related to the build process of the generated applications. */
export interface RunSummaryBuilds {
  /** The number of applications that built successfully on the very first attempt. */
  successfulInitialBuilds: number;
  /** The number of applications that built successfully after one or more repair attempts. */
  successfulBuildsAfterRepair: number;
  /** The number of applications that don't have a successful build even after repair attempts. */
  failedBuilds: number;
  /** Distribution of error types for failed builds. */
  errorDistribution?: Partial<Record<BuildErrorType, number>>;
}

/** Buckets into which scores can be categorized. */
export interface ScoreBucket {
  /** Plain name of the bucket, e.g. "Good" */
  name: string;
  /** ID for the bucket, e.g. "good" */
  id: string;
  /** Name of the bucket with labels, e.g. "Good (50-75%)" */
  nameWithLabels: string;
  /** Minimum value that a score should have in order to be placed in the bucket. */
  min: number;
  /** Maximum value that a score should have in order to be placed in the bucket. */
  max: number;
  /** Number of apps in the bucket. */
  appsCount: number;
}

/**
 * Represents statistics related to runtime errors of the generated applications.
 */
export interface RuntimeStats {
  /** The number of applications that encountered one or more runtime errors. */
  appsWithErrors: number;
  /** The number of applications that ran without any runtime errors. */
  appsWithoutErrors: number;
}

/** Represents aggregated statistics for builds and checks in a run. */
export interface AggregatedRunStats {
  /** Statistics related to the build process of the generated applications. */
  builds: RunSummaryBuilds;
  /** Statistics about generated apps organized in buckets. */
  buckets: ScoreBucket[];
  /** Runtime stats. Not present for reports that didn't request runtime error collection. */
  runtime?: RuntimeStats;

  accessibility?: {
    appsWithErrors: number;
    appsWithoutErrorsAfterRepair: number;
    appsWithoutErrors: number;
  };
  security?: { appsWithErrors: number; appsWithoutErrors: number };
}

export interface CompletionStats {
  /** How many root prompts are part of this run. */
  allPromptsCount: number;
  /** How many root prompts failed completion as part of this run. */
  failedPrompts: Array<{
    promptName: string;
    error: string;
    stack: string | undefined;
  }>;
}

export interface FrameworkInfo {
  /** Unique ID of the framework. */
  id: string;
  /** Display name of the framework. */
  displayName: string;
}

/**
 * A summary of build outcomes and code quality scores for an entire assessment run.
 */
export interface RunSummary {
  /** Usage statistics for the entire run. */
  usage: Usage;
  /** Display name for the run. */
  displayName: string;
  /** Unique ID of the environment. */
  environmentId: string;
  /** Info about the framework that was used for this run. */
  framework: {
    /** Name of the full-stack framework used for the run. */
    fullStackFramework: FrameworkInfo;
    /** Name of the client-side framework used for the run. */
    clientSideFramework: FrameworkInfo;
  };
  /** Name of the model used for the run */
  model: string;
  /** Errors produced by the external API. */
  responseErrors?: LlmResponseError[];
  /** Stats about the overall run */
  completionStats?: CompletionStats;
  /** AI summary (as HTML code) of all assessments in this run/report. */
  aiSummary?: string;
  /**
   * Information about the runner that was used for the eval.
   * Optional since some older reports might not have it.
   */
  runner?: CodegenRunnerInfo;
}

/**
 * Contains detailed information about an assessment run, including a summary
 * and the specific prompts used for system instructions and best practices,
 * as well as the time the run was initiated.
 */
export interface RunDetails {
  /** A summary of build and score statistics for the run. */
  summary: RunSummary;
  /** An ISO string representing the date and time when the assessment run was initiated. */
  timestamp: string;
  /** The name of the report, derived from CLI arguments or a default value. */
  reportName: string;
  /**
   * System prompt used for generation requests to the LLM.
   */
  systemPromptGeneration: string;
  /**
   * System prompt used for repair requests to the LLM.
   */
  systemPromptRepair: string;
  /**
   * Metadata labels that can be added to runs for easier identification later.
   *
   * E.g. running evals with various models configurations and pulling them later
   * for a comparison.
   */
  labels?: string[];

  /** Information about configured MCP servers, if any. */
  mcp?: {
    /** MCP servers that were configured. */
    servers: { name: string; command: string; args: string[] }[];

    /** Logs produced by all of the servers. */
    logs: string;
  };

  // TODO: Migrate older reports to the new fields, or remove these simply.
  /** The text of the best practices prompt used during the run. */
  bestPracticesPrompt?: string;
  /** The text of the system instructions prompt used during the run. */
  systemInstructionsPrompt?: string;
}

/**
 * Logs for a single tool request and response (e.g. an MCP tool).
 *
 * Fields are coming from GenerateRequestSchema.
 */
export interface ToolLogEntry {
  request: {
    name: string;
    ref?: string | undefined;
    input?: unknown;
  };
  response: {
    name: string;
    output?: unknown;
    ref?: string | undefined;
  };
}

/**
 * Encapsulates all results and details for the assessment of a single prompt.
 * This includes the original prompt definition, the final generated code,
 * build status, code quality score, number of repair attempts, and details of each attempt.
 */
export interface AssessmentResult {
  /** The definition of the prompt that was assessed. */
  promptDef: Pick<PromptDefinition, 'name' | 'prompt'>;
  /** The final version of the generated source code after all attempts. */
  outputFiles: LlmResponseFile[];
  /** The final build result for the generated code. */
  build: BuildResult;
  /** The code quality assessment score. */
  score: CodeAssessmentScore;
  /** The number of repair attempts made after the initial code generation. */
  repairAttempts: number;
  /** An array detailing each attempt (initial and repairs) made for this prompt. */
  attemptDetails: AttemptDetails[];
  /** Pre-computed user journeys. */
  userJourneys?: UserJourneysResult;
  /** The number of repair attempts made after the axe initial failures. */
  axeRepairAttempts: number;
  /** Tool requests logs (e.g. MCP requests and responses). */
  toolLogs: ToolLogEntry[];
}

/**
 * Represents all information for a complete assessment run,
 * encompassing the results for all assessed prompts and the overall run details.
 */
export interface RunInfo {
  /** Unique ID of the run. */
  id: string;
  /** Identifier that can be used to group similar reports together. */
  group: string;
  /** Version of the report. */
  version?: typeof REPORT_VERSION;
  /** An array of assessment results, one for each prompt processed in the run. */
  results: AssessmentResult[];
  /** Detailed information about the run, including summaries and prompts used. */
  details: RunDetails;
}

/** Autorater run information */
export interface AutoraterRunInfo {
  /** The model that was used for scoring. */
  model: string;
  /** Ratings for the code. */
  codeRating: AutoRateResult;
  /** Visual ratings for the run. */
  visualRating?: AutoRateResult;
}

/** Information about a runner of codegen evals. */
export interface CodegenRunnerInfo {
  /** Unique ID of the runner. */
  id: string;
  /** Formatted display name of the runner. */
  displayName: string;
}

/**
 * Represents summarized information about a group of reports.
 * Useful when displaying a list of reports.
 */
export interface RunGroup {
  /** Unique ID of the group. */
  id: string;
  /** Version of the group structure. */
  version: typeof REPORT_VERSION;
  /** Formatted display name for the group. */
  displayName: string;
  /** Timestamp at which the group was created. */
  timestamp: string;
  /** Averaged total number of points across all runs in the group. */
  totalPoints: number;
  /** Averaged maximum number of points across all runs in the group. */
  maxOverallPoints: number;
  /** Labels from all the runs in the group. */
  labels: string[];
  /** ID of the environment of the reports in the group. */
  environmentId: string;
  /** Model used for the runs in the group. */
  model: string;
  /** Total apps generated within the group. */
  appsCount: number;
  /** Aggregated stats about all runs in the group. */
  stats: AggregatedRunStats;
  /** Framework used for the runs in the group. */
  framework: {
    fullStackFramework: FrameworkInfo;
    clientSideFramework: FrameworkInfo;
  };
  /** Runner used to generate code for the runs in the group. */
  runner?: CodegenRunnerInfo;
}
