# Web Codegen Scorer CLI Reference

## Overview

**Web Codegen Scorer** is a tool for evaluating the quality of web code generated by Large Language Models (LLMs). It helps you make evidence-based decisions about AI-generated code by providing comprehensive scoring and reporting capabilities.

## Installation

```bash
npm install -g web-codegen-scorer
```

## API Keys Setup

Before running evaluations, set up API keys for your chosen providers:

```bash
export GEMINI_API_KEY="YOUR_API_KEY_HERE"      # For Gemini models
export OPENAI_API_KEY="YOUR_API_KEY_HERE"      # For OpenAI models
export ANTHROPIC_API_KEY="YOUR_API_KEY_HERE"   # For Anthropic models
export XAI_API_KEY="YOUR_API_KEY_HERE"         # For xAI Grok models
```

---

## Commands

### 1. `eval` - Evaluate Code Using an LLM

Runs code generation and quality assessment.

#### Usage

```bash
web-codegen-scorer eval [options]
```

#### Required Options

- `--env=<path>`, `--environment=<path>`
  - Path to environment configuration file or built-in environment name
  - Examples: `--env=angular-example`, `--env=./my-env/config.mjs`

#### Model Configuration

- `--model=<name>`
  - Model to use for code generation
  - Default: `gemini-2.0-flash-exp` (or runner-specific default)
  - Examples: `gemini-2.5-flash`, `gpt-4`, `claude-3-5-sonnet`

- `--autorater-model=<name>`
  - Model to use for automatic code rating
  - Default: `gemini-2.0-flash-exp`

- `--runner=<name>`
  - Runner to execute the evaluation
  - Choices: `genkit` (default), `gemini-cli`, `codex-cli`, `claude-code-cli`

- `--autorater-runner=<name>`
  - Runner for autorater and AI summary
  - Choices: Same as `--runner`
  - Default: Uses main runner value

#### Execution Control

- `--local`
  - Use locally-cached LLM output instead of calling LLM
  - Reads from `.web-codegen-scorer/llm-output` directory
  - Useful for re-running assessments without LLM costs
  - Default: `false`

- `--limit=<number>`
  - Maximum number of apps to generate and assess
  - Default: `5`

- `--concurrency=<number|'auto'>`
  - Maximum concurrent evaluations
  - Default: `'auto'`

- `--prompt-filter=<string>`
  - Filter which prompts to run by name
  - Useful for debugging specific prompts
  - Example: `--prompt-filter=tic-tac-toe`

#### Output Configuration

- `--output-directory=<path>`, `--output-dir=<path>`
  - Directory for generated code (for debugging)
  - Default: Temporary directory

- `--report-name=<name>`
  - Name for generated report directory
  - Default: Timestamp (e.g., `2023-10-27T10-30-00-000Z`)

- `--labels=<label1> <label2> ...`
  - Metadata labels attached to the run
  - Example: `--labels experiment-1 baseline`

#### Feature Toggles

- `--mcp`
  - Start Model Context Protocol server for evaluation
  - Default: `false`

- `--skip-screenshots`
  - Skip taking screenshots of generated apps
  - Default: `false`

- `--skip-ai-summary`
  - Skip generating AI summary for report
  - Default: `false`

- `--skip-axe-testing`
  - Skip Axe accessibility testing
  - Default: `false`

- `--enable-user-journey-testing`, `--user-journeys`
  - Enable user journey testing via browser automation
  - Default: `false`

- `--enable-auto-csp`
  - Include automatic hash-based CSP and Trusted Types
  - Default: `false`

#### Advanced Options

- `--rag-endpoint=<url>`
  - Custom RAG endpoint URL
  - Must contain `PROMPT` substring (replaced with user prompt)
  - Example: `--rag-endpoint="http://localhost:8080/rag?query=PROMPT"`

- `--logging=<type>`
  - Logging type during evaluation
  - Choices: `text-only`, `dynamic`
  - Default: `dynamic` (or `text-only` when `CI=1`)

#### Examples

```bash
# Basic evaluation with built-in Angular example
web-codegen-scorer eval --env=angular-example

# Custom environment with specific model
web-codegen-scorer eval --env=./my-config.mjs --model=claude-3-5-sonnet

# Limited run with custom concurrency
web-codegen-scorer eval --env=angular-example --limit=10 --concurrency=3

# Debug specific prompt without screenshots
web-codegen-scorer eval --env=solid-example --prompt-filter=todo-app --skip-screenshots

# Local mode (reuse cached output)
web-codegen-scorer eval --env=angular-example --local

# Full evaluation with all features
web-codegen-scorer eval \
  --env=./config.mjs \
  --model=gemini-2.5-flash \
  --autorater-model=gemini-2.5-pro \
  --limit=20 \
  --concurrency=5 \
  --enable-user-journey-testing \
  --labels experiment-2 optimized \
  --report-name=my-experiment
```

---

### 2. `run` - Run an Evaluated App Locally

Runs a previously evaluated application locally for inspection.

#### Usage

```bash
web-codegen-scorer run [options]
```

#### Required Options

- `--env=<path>`, `--environment=<path>`
  - Path to environment configuration file
  - Example: `--env=angular-example`

- `--prompt=<name>`
  - ID of the prompt to run
  - Must match a prompt that was previously evaluated
  - Example: `--prompt=todo-app`

#### Examples

```bash
# Run evaluated todo app
web-codegen-scorer run --env=angular-example --prompt=todo-app

# Run with custom environment
web-codegen-scorer run --env=./my-config.mjs --prompt=contact-form
```

---

### 3. `init` - Initialize New Environment

Interactive guide for creating a new evaluation environment.

#### Usage

```bash
web-codegen-scorer init
```

#### Interactive Prompts

1. **Environment name**: Display name for the environment
2. **Config file path**: Where to place the config file (must be `.js` or `.mjs`)
3. **Client-side framework**: Framework being used (e.g., React, Angular, Vue)
4. **Source directory**: Root directory for code generation (where `package.json` is)
5. **System instructions**: Optional path to system instructions file
6. **Executable prompts**: Optional glob pattern for prompt files

#### Generated Files

- Environment configuration file
- Example system instructions (if not provided)
- Example prompts (if not provided)

#### Example Session

```bash
$ web-codegen-scorer init

┌─────────────────────────────────────────────────────────┐
│ Welcome LLM enthusiast! 🎉                              │
│ Answer the following questions to create an eval        │
│ environment                                              │
└─────────────────────────────────────────────────────────┘

? What will be the name of your environment? My React App
? Where should we place the environment config file? my-react-app/config.mjs
? What client-side framework will it be using? React
? In which directory should the LLM generate and execute code? ./src
? What file contains your system instructions? (leave blank for example)
? What prompts should the LLM execute? (leave blank for examples)

✓ Configuration created at my-react-app/config.mjs
```

---

### 4. `report` - View Evaluation Reports

Launches web UI to view and compare evaluation reports.

#### Usage

```bash
web-codegen-scorer report [options]
```

#### Options

- `--reports-directory=<path>`
  - Path to read local reports from
  - Default: `.web-codegen-scorer/reports`

- `--reports-loader=<path>`
  - Path to JavaScript file for loading remote reports
  - Optional

- `--port=<number>`
  - Port for serving report UI
  - Default: `4200`

#### Examples

```bash
# View reports with defaults
web-codegen-scorer report

# Custom reports directory
web-codegen-scorer report --reports-directory=./my-reports

# Custom port
web-codegen-scorer report --port=8080

# Remote reports loader
web-codegen-scorer report --reports-loader=./load-reports.js
```

---

## Environment Configuration

### Configuration File Structure

Environment configs are `.mjs` or `.js` files that export a configuration object:

```javascript
import { getBuiltInRatings } from 'web-codegen-scorer';

/** @type {import("web-codegen-scorer").EnvironmentConfig} */
export default {
  // Required
  displayName: 'My Environment',
  clientSideFramework: 'react',
  sourceDirectory: './src',
  ratings: [...getBuiltInRatings()],
  generationSystemPrompt: './system-instructions.md',
  executablePrompts: ['./prompts/**/*.md'],

  // Optional
  id: 'my-env',
  packageManager: 'npm',
  skipInstall: false,
  buildCommand: 'npm run build',
  serveCommand: 'npm run start -- --port 0',
  mcpServers: [],
  repairSystemPrompt: './repair-instructions.md',
  editingSystemPrompt: './editing-instructions.md',
  codeRatingPrompt: './rating-prompt.md',
  classifyPrompts: false,
  projectTemplate: './template',
  fullStackFramework: 'next',
};
```

### Built-in Environments

- `angular-example`: Angular application example
- `solid-example`: SolidJS application example
- `remote_env`: Remote environment example

### Prompt Templating

Prompts support Handlebars-style templating:

```markdown
{{> embed file='../shared/common-instructions.md' }}

{{> contextFiles '**/*.ts, **/*.html' }}

Use {{CLIENT_SIDE_FRAMEWORK_NAME}} for this component.
```

### Multi-Step Prompts

```javascript
import { MultiStepPrompt } from 'web-codegen-scorer';

executablePrompts: [
  new MultiStepPrompt('./prompts/about-page', {
    'step-1': ratingsForFirstStep,
    'step-2': [...ratingsForFirstStep, ...ratingsForSecondStep],
  }),
];
```

Each step file: `step-1.md`, `step-2.md`, etc.

---

## Built-in Ratings

The tool provides several built-in code quality ratings:

1. **PerBuildRating**: Scores based on build success
2. **PerFileRating**: Analyzes individual file content
3. **LLMBasedRating**: Uses LLM to rate code quality
4. **AxeRating**: Accessibility testing
5. **Code Quality Ratings**: Various best practices checks

Access via:

```javascript
import { getBuiltInRatings } from 'web-codegen-scorer';
```

---

## Supported Models

### Genkit (Default Runner)

- Gemini: `gemini-2.0-flash-exp`, `gemini-2.5-flash`, `gemini-2.5-pro`
- OpenAI: `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`
- Anthropic: `claude-3-5-sonnet`, `claude-3-opus`
- xAI: `grok-beta`

### CLI Runners

- **gemini-cli**: Google Gemini CLI
- **codex-cli**: OpenAI Codex CLI (default: `gpt-5-codex`)
- **claude-code-cli**: Anthropic Claude Code CLI (default: `claude-4.5-sonnet`)

---

## Output Structure

```
.web-codegen-scorer/
├── llm-output/           # Cached LLM responses
│   └── <env-id>/
│       └── <prompt-name>/
│           └── [generated files]
└── reports/              # Evaluation reports
    └── <report-name>/
        ├── report.json   # Report data
        ├── summary.json  # Summary statistics
        └── screenshots/  # App screenshots
```

---

## CLI Shortcuts

The tool provides shorter aliases:

```bash
# Short package name
wcs eval --env=angular-example

# Equivalent to
web-codegen-scorer eval --env=angular-example
```

---

## Exit Codes

- `0`: Success
- `1`: Error (with error message displayed)

---

## Debugging Tips

1. **Use `--output-dir`** to inspect generated code
2. **Use `--prompt-filter`** to test specific prompts
3. **Use `--local`** to rerun without LLM costs
4. **Use `--skip-screenshots`** for faster iterations
5. **Check `.web-codegen-scorer/llm-output`** for cached responses

---

## Common Workflows

### Iterate on System Prompt

```bash
# Edit system-instructions.md
# Run evaluation
web-codegen-scorer eval --env=./config.mjs --limit=5

# View results
web-codegen-scorer report
```

### Compare Models

```bash
# Run with Model A
web-codegen-scorer eval --env=./config.mjs --model=gemini-2.5-flash --report-name=model-a

# Run with Model B
web-codegen-scorer eval --env=./config.mjs --model=claude-3-5-sonnet --report-name=model-b

# Compare in UI
web-codegen-scorer report
```

### Debug Specific Prompt

```bash
# Run specific prompt with output directory
web-codegen-scorer eval \
  --env=./config.mjs \
  --prompt-filter=contact-form \
  --output-dir=./debug-output \
  --skip-screenshots

# Inspect generated code
cd debug-output

# Run the app locally
web-codegen-scorer run --env=./config.mjs --prompt=contact-form
```

---

## Version

Package version: `0.0.10`

---

## Links

- GitHub: https://github.com/angular/web-codegen-scorer
- Issues: https://github.com/angular/web-codegen-scorer/issues
- NPM: https://www.npmjs.com/package/web-codegen-scorer
